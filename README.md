# Stacked Generalisation for MNIST
---
A ***Ensemble Learning*** is inspired by the concept of the *Wisdom of the Crowd* - the idea that the combined knowledge of many individuals is better than that of one. In the context of Machine Learning, an ensemble can be build by using a number of unique predictors, whose predictions are combined to produce the final result. **These tend to perform better than standard predictors.** 

Some of the most popularly used *ensemble methods* (ensemble learning algorithms) include:
* [Voting Classifiers]()
* [Bagging and Pasting]()
* [Random Forests]()
* [Boosting - AdaBoost and Gradient Boosting]()
* [**Stacking**]()

## Motivations
**This project is for purely intellectual purposes**, and any conclusions and findings made here may not be directly application for analysis of data other than MNIST. These findings should have give readers a better initution as to how a stacking ensemble works and out performs non-stacking based algorithms. 

A stacked ensemble consists of many tuned models also, so a deeper understanding of a variety of different machine learning models will be required, and gained, over the duration of this project. In proffessional projects, ensemble methods tend to be used towards the end of the model build and tuning phase, after a number of model have been short listed to be used.

Hence, the main goals of this analysis/project will be:
1. to tune and make predictions using several non-ensemble predictors;
2. to construct and tune a stacked ensemble;
3. to compare the performace of a hand-build stacked ensemble to ensembles provided by scikit-learn;
4. to compare the performance of a hand-build stacked ensemble to existing open source implementations such as [brew]();


## Technical Details
### Ensemble Models


### Stacking


### Libraries


## References 
[] Hands-on Machine Learning with Tensorflow and Scikit-Learn
