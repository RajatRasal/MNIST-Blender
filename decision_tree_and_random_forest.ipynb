{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.style.use(\"Solarize_Light2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatrasal/anaconda3/envs/crypto_time_series/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/rajatrasal/anaconda3/envs/crypto_time_series/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'DESCR': 'mldata.org dataset: mnist-original',\n",
       " 'COL_NAMES': ['label', 'data'],\n",
       " 'target': array([0., 0., 0., ..., 9., 9., 9.]),\n",
       " 'data': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "mnist = fetch_mldata('MNIST Original')\n",
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (52500, 784) X_test: (17500, 784),\n",
      "y_train: (52500,) y_test: (17500,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = mnist['data'], mnist['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(f\"\"\"X_train: {X_train.shape} X_test: {X_test.shape},\n",
    "y_train: {y_train.shape} y_test: {y_test.shape}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (47250, 784) X_val: (5250, 784),\n",
      "y_train: (47250,) y_val: (5250,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "print(f\"\"\"X_train: {X_train.shape} X_val: {X_val.shape},\n",
    "y_train: {y_train.shape} y_val: {y_val.shape}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a white box model in that it is fairly easy to interpret the decisions being made**[x]**. \n",
    "\n",
    "Each node in the decision tree has a number of attributes:\n",
    "* **Samples** - then number of training instances applicable at that node.\n",
    "* **Value** - the number of training instances of each class this node applies to, e.g. [0, 10, 20], if there are 3 classes we are classifying by.\n",
    "* **Impurity** - a measure of if all training instances this node apply to fall under the same class. This is done by either calculating the gini or entropy of the node. This is the value which is being minimised when optimising the decision tree.\n",
    "\n",
    "![IMAGE OF DECISION TREE FROM GRAPHVIZ]()\n",
    "\n",
    "**Predictions** are made by following the decision tree down to the leaves for a particular training instance. At the leaf, we look at the *value* attribute. The prediction is then made as follows:\n",
    "\n",
    "```\n",
    "# Find the probabilities of each class being the correct.\n",
    "percent_proba = map(enumerate(value), lambda i, x: (i, x/sum(value)))\n",
    "# Select the class with the highest probability.\n",
    "class, percent = max(percent_proba, key=lambda x: x[1])\n",
    "```\n",
    "\n",
    "**Impurity** is determined by 2 main methods:\n",
    "* *Entropy* - based on Shannon's Information Theory. We use this is a measure of the average information content of a message, hence entropy is 0 when a node only applies to values of 1 class. $$H_i = - \\sum_{k=1}^n \\space p_{i, k} log(p_{i, k})$$ where $p_{i, k} =$ ```nodes[i].values[k] / sum(nodes[i].values)```.\n",
    "* *Gini* - This purely looks at the class probabilities. $$G_i = 1 - \\sum_{k=1}^n \\space (p_{i, k})^2$$ where $p_{i, k}$ is defined in the same way as for entropy.\n",
    "\n",
    "In most cases, it doesn't matter which model loss function we pick. Gini is slightly faster but in some cases entropy can result in more balanced trees.\n",
    "\n",
    "**CART**\n",
    "Classification and Regression Tree Algorithm (CART) is the training algorithm used by scikit-learn. At each node, the algorithm first splits the dataset into 2 sets by choosing a single feature $k$ and a threshold $t_k$, based on which $(k, t_k)$ pair minimise the impurity the most.\n",
    "\n",
    "$$J(k, t_k) = \\frac{m_{left}}{m}I_{left} + \\frac{m_{right}}{m}I_{right}$$\n",
    "\n",
    "This continued until we reach a specified max depth.\n",
    "\n",
    "---\n",
    "[x] This is in contract to neural network where there is no clear or intuitive way to decide why a feature training decision was made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "Unlike SVMs, we want to reduce the feature space of the decision tree, or else the CART algorithm will overfit the training set**[]**. We can do this using PCA.\n",
    "\n",
    "From Tensorflow and Sklearn book, to preserve about 95% of MNIST's variance, we need to reduce its feature space down from 784 to 150.\n",
    "\n",
    "---\n",
    "[] With an SVM we often want to increase the feature space, by using an RBF kernel for example, so as to the increase the chances of being able to fit a linear model which accurately fits the training set. PCA may still be useful as a compression method to speed up the overall computations of an RBF kernel or linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=154)\n",
    "# Do the image comparison to show how the compressed image is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.8220740740740741\n",
      "val accuracy: 0.708952380952381\n"
     ]
    }
   ],
   "source": [
    "dt_clf = Pipeline([\n",
    "    (\"pca\", PCA(n_components=154)),\n",
    "    (\"clf\", DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "params = {\"clf__max_depth\": 20}\n",
    "\n",
    "dt_clf_1 = dt_clf.set_params(**params)\n",
    "dt_clf_1.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = cross_val_predict(dt_clf_1, X_train, y_train, cv=5)\n",
    "y_val_pred = cross_val_predict(dt_clf_1, X_val, y_val, cv=5)\n",
    "print(f\"train accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
    "print(f\"val accuracy: {accuracy_score(y_val, y_val_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[] Gini vs Entropy loss functions - https://sebastianraschka.com/faq/docs/decision-tree-binary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:crypto_time_series]",
   "language": "python",
   "name": "conda-env-crypto_time_series-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
